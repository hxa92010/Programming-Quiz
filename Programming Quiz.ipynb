{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (13386, 10, 3)\n",
      "y_train shape: (13386,)\n",
      "X_test shape: (3347, 10, 3)\n",
      "y_test shape: (3347,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('weather.csv')\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)  # Drop rows with missing values\n",
    "\n",
    "# Select relevant features and target variable\n",
    "# Example: assuming 'temperature' is the target variable and other columns are features\n",
    "features = data[['Data.Temperature.Avg Temp', 'Data.Temperature.Max Temp', 'Data.Temperature.Min Temp']]  # Select features\n",
    "target = data['Data.Precipitation']  # Select target variable\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Define sequence length (time steps) for LSTM model\n",
    "seq_length = 10  # Example: use 10 time steps\n",
    "\n",
    "# Reshape data into sequences\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(scaled_features) - seq_length):\n",
    "    X.append(scaled_features[i:i+seq_length])\n",
    "    y.append(target[i+seq_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Print shapes of training and test sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10, 64)            17408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 64)            33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 83,521\n",
      "Trainable params: 83,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# Define the LSTM-based architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first LSTM layer with dropout\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add the second LSTM layer with dropout\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add the third LSTM layer with dropout\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Display the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13386 samples, validate on 3347 samples\n",
      "Epoch 1/10\n",
      "13386/13386 [==============================] - 18s 1ms/step - loss: 1.0484 - val_loss: 0.6993\n",
      "Epoch 2/10\n",
      "13386/13386 [==============================] - 12s 933us/step - loss: 1.0397 - val_loss: 0.6938\n",
      "Epoch 3/10\n",
      "13386/13386 [==============================] - 16s 1ms/step - loss: 1.0384 - val_loss: 0.6898\n",
      "Epoch 4/10\n",
      "13386/13386 [==============================] - 13s 943us/step - loss: 1.0351 - val_loss: 0.6870\n",
      "Epoch 5/10\n",
      "13386/13386 [==============================] - 11s 815us/step - loss: 1.0339 - val_loss: 0.7064\n",
      "Epoch 6/10\n",
      "13386/13386 [==============================] - 11s 794us/step - loss: 1.0348 - val_loss: 0.6850\n",
      "Epoch 7/10\n",
      "13386/13386 [==============================] - 10s 750us/step - loss: 1.0309 - val_loss: 0.6844\n",
      "Epoch 8/10\n",
      "13386/13386 [==============================] - 9s 665us/step - loss: 1.0299 - val_loss: 0.6851\n",
      "Epoch 9/10\n",
      "13386/13386 [==============================] - 9s 663us/step - loss: 1.0305 - val_loss: 0.6872\n",
      "Epoch 10/10\n",
      "13386/13386 [==============================] - 10s 763us/step - loss: 1.0258 - val_loss: 0.6827\n",
      "Train on 13386 samples, validate on 3347 samples\n",
      "Epoch 1/10\n",
      "13386/13386 [==============================] - 22s 2ms/step - loss: 1.0512 - val_loss: 0.6891\n",
      "Epoch 2/10\n",
      "13386/13386 [==============================] - 20s 1ms/step - loss: 1.0430 - val_loss: 0.6878\n",
      "Epoch 3/10\n",
      "13386/13386 [==============================] - 16s 1ms/step - loss: 1.0390 - val_loss: 0.7027\n",
      "Epoch 4/10\n",
      "13386/13386 [==============================] - 18s 1ms/step - loss: 1.0357 - val_loss: 0.6865\n",
      "Epoch 5/10\n",
      "13386/13386 [==============================] - 21s 2ms/step - loss: 1.0315 - val_loss: 0.6819\n",
      "Epoch 6/10\n",
      "13386/13386 [==============================] - 18s 1ms/step - loss: 1.0303 - val_loss: 0.6947\n",
      "Epoch 7/10\n",
      "13386/13386 [==============================] - 18s 1ms/step - loss: 1.0249 - val_loss: 0.6762\n",
      "Epoch 8/10\n",
      "13386/13386 [==============================] - 19s 1ms/step - loss: 1.0212 - val_loss: 0.6746\n",
      "Epoch 9/10\n",
      "13386/13386 [==============================] - 19s 1ms/step - loss: 1.0186 - val_loss: 0.6823\n",
      "Epoch 10/10\n",
      "13386/13386 [==============================] - 17s 1ms/step - loss: 1.0166 - val_loss: 0.6722\n",
      "Train on 13386 samples, validate on 3347 samples\n",
      "Epoch 1/10\n",
      "13386/13386 [==============================] - 45s 3ms/step - loss: 1.0538 - val_loss: 0.6891\n",
      "Epoch 2/10\n",
      "13386/13386 [==============================] - 41s 3ms/step - loss: 1.0462 - val_loss: 0.6898\n",
      "Epoch 3/10\n",
      "13386/13386 [==============================] - 39s 3ms/step - loss: 1.0453 - val_loss: 0.6893\n",
      "Epoch 4/10\n",
      "13386/13386 [==============================] - 41s 3ms/step - loss: 1.0416 - val_loss: 0.6882\n",
      "Epoch 5/10\n",
      "13386/13386 [==============================] - 37s 3ms/step - loss: 1.0345 - val_loss: 0.6821\n",
      "Epoch 6/10\n",
      "13386/13386 [==============================] - 40s 3ms/step - loss: 1.0299 - val_loss: 0.6783\n",
      "Epoch 7/10\n",
      "13386/13386 [==============================] - 42s 3ms/step - loss: 1.0244 - val_loss: 0.6804\n",
      "Epoch 8/10\n",
      "13386/13386 [==============================] - 41s 3ms/step - loss: 1.0218 - val_loss: 0.6850\n",
      "Epoch 9/10\n",
      "13386/13386 [==============================] - 50s 4ms/step - loss: 1.0191 - val_loss: 0.7065\n",
      "Epoch 10/10\n",
      "13386/13386 [==============================] - 41s 3ms/step - loss: 1.0104 - val_loss: 0.6757\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# Define baseline model with a single LSTM layer\n",
    "baseline_model = Sequential()\n",
    "baseline_model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "baseline_model.add(Dropout(0.2))\n",
    "baseline_model.add(Dense(units=1))\n",
    "baseline_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train and evaluate the baseline model\n",
    "baseline_history = baseline_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Experiment with stacking multiple LSTM layers and adjusting the number of units\n",
    "models = []\n",
    "\n",
    "# Model 1: Two LSTM layers with 64 units each\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model1.add(LSTM(units=64))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(units=1))\n",
    "model1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "models.append(model1)\n",
    "\n",
    "# Model 2: Three LSTM layers with 128 units each\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model2.add(LSTM(units=128, return_sequences=True))\n",
    "model2.add(LSTM(units=128))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(units=1))\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "models.append(model2)\n",
    "\n",
    "# Train and evaluate each model\n",
    "histories = []\n",
    "for model in models:\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "    histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Calculate root mean squared error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a function to create the LSTM model\n",
    "def create_model(learning_rate=0.001, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=num_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Create KerasRegressor wrapper for use with scikit-learn\n",
    "model = KerasRegressor(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_units': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_result.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_result.best_estimator_\n",
    "loss = best_model.score(X_test, y_test)\n",
    "print(\"Test Loss (MSE) with Best Model:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning using Grid Search\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a function to create the LSTM model\n",
    "def create_model(learning_rate=0.001, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=num_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Create KerasRegressor wrapper for use with scikit-learn\n",
    "model = KerasRegressor(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_units': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_result.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_result.best_estimator_\n",
    "loss = best_model.score(X_test, y_test)\n",
    "print(\"Test Loss (MSE) with Best Model:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Discuss the challenges encountered during model training and\n",
    "optimization.\n",
    "Ans: Selecting the optimal set of hyperparameters (e.g., learning rate, dropout rate, number of units) for an LSTM model can be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
